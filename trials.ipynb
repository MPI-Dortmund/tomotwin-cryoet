{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tomotwin.modules.networks.torchmodel import TorchModel\n",
    "\n",
    "class AutoEncoder(TorchModel):\n",
    "\n",
    "    NORM_BATCHNORM = \"BatchNorm\"\n",
    "    NORM_GROUPNORM = \"GroupNorm\"\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        def make_norm(self, norm: Dict, num_channels: int) -> nn.Module:\n",
    "            if norm[\"module\"] == nn.BatchNorm3d:\n",
    "                norm[\"kwargs\"][\"num_features\"] = num_channels\n",
    "                return norm[\"module\"](**norm[\"kwargs\"])\n",
    "            elif norm[\"module\"] == nn.GroupNorm:\n",
    "                norm[\"kwargs\"][\"num_channels\"] = num_channels\n",
    "                return norm[\"module\"](**norm[\"kwargs\"])\n",
    "            else:\n",
    "                raise ValueError(\"Not supported norm\", norm[\"module\"])\n",
    "\n",
    "\n",
    "        def __init__(\n",
    "            self,\n",
    "            output_channels: int,\n",
    "            norm: Dict,\n",
    "            dropout: float = 0.5,\n",
    "            repeat_layers=0,\n",
    "            gem_pooling = None,\n",
    "        ):\n",
    "            super().__init__()\n",
    "            norm_func = self.make_norm(norm, 64)\n",
    "            self.en_layer0 = self._make_conv_layer(1, 64, norm=norm_func)\n",
    "\n",
    "            norm_func = self.make_norm(norm, 128)\n",
    "            self.en_layer1 = self._make_conv_layer(64, 128, norm=norm_func)\n",
    "\n",
    "            norm_func = self.make_norm(norm, 256)\n",
    "            self.en_layer2 = self._make_conv_layer(128, 256, norm=norm_func)\n",
    "\n",
    "            norm_func = self.make_norm(norm, 512)\n",
    "            self.en_layer3 = self._make_conv_layer(256, 512, norm=norm_func)\n",
    "\n",
    "\n",
    "            self.max_pooling = nn.MaxPool3d((2, 2, 2))\n",
    "            if gem_pooling:\n",
    "                self.adap_max_pool = gem_pooling\n",
    "            else:\n",
    "                self.adap_max_pool = nn.AdaptiveAvgPool3d((2, 2, 2))\n",
    "            \n",
    "            self.headnet = self._make_headnet(\n",
    "                 512, 256, 64, 1, dropout=dropout\n",
    "            )\n",
    "\n",
    "            norm_func = self.make_norm(norm, 256)\n",
    "            self.de_layer0 = self._make_deconv_layer(512, 256, norm=norm_func)\n",
    "\n",
    "            norm_func = self.make_norm(norm, 128)\n",
    "            self.de_layer1 = self._make_deconv_layer(256, 128, norm=norm_func)\n",
    "\n",
    "            norm_func = self.make_norm(norm, 64)\n",
    "            self.de_layer2 = self._make_deconv_layer(128, 64, norm=norm_func)\n",
    "\n",
    "            #norm_func = self.make_norm(norm, 64)\n",
    "            #self.de_layer3 = self._make_conv_layer(128, 64, norm=norm_func)\n",
    "\n",
    "            #norm_func = self.make_norm(norm, 1)\n",
    "            #self.de_layer4 = self._make_conv_layer(64, 1, norm=norm_func)\n",
    "            self.de_layer4 = nn.Sequential(\n",
    "                nn.ConvTranspose3d(64, 1, kernel_size=3, padding=1),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.ConvTranspose3d(1, 1, kernel_size=3, padding=1),\n",
    "                nn.Identity() \n",
    "            )\n",
    "\n",
    "            self.up_sampling = nn.Upsample(scale_factor =2)\n",
    "\n",
    "        @staticmethod\n",
    "        def _make_conv_layer(in_c: int, out_c: int, norm: nn.Module, padding: int = 1, kernel_size: int =3):\n",
    "            conv_layer = nn.Sequential(\n",
    "                nn.Conv3d(in_c, out_c, kernel_size=3, padding=padding),\n",
    "                norm,\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Conv3d(out_c, out_c, kernel_size=3, padding=padding),\n",
    "                norm,\n",
    "                nn.LeakyReLU(),\n",
    "            )\n",
    "            return conv_layer\n",
    "        \n",
    "        @staticmethod\n",
    "        def _make_deconv_layer(in_c: int, out_c: int, norm: nn.Module, padding: int = 1, kernel_size: int =3):\n",
    "            conv_layer = nn.Sequential(\n",
    "                nn.ConvTranspose3d(in_c, out_c, kernel_size=3, padding=padding),\n",
    "                norm,\n",
    "                nn.LeakyReLU(),\n",
    "                nn.ConvTranspose3d(out_c, out_c, kernel_size=3, padding=padding),\n",
    "                norm,\n",
    "                nn.LeakyReLU(),\n",
    "            )\n",
    "            return conv_layer\n",
    "\n",
    "        @staticmethod\n",
    "        def _make_headnet(\n",
    "            in_c1: int, in_c2: int,out_c1: int, out_head: int, dropout: float\n",
    "        ) -> nn.Sequential:\n",
    "            headnet = nn.Sequential(\n",
    "                nn.Dropout(p=dropout),\n",
    "                nn.Conv3d(in_c1, in_c2, kernel_size=3, padding=1),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Conv3d(in_c2, out_c1, kernel_size=3, padding=1),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Conv3d(out_c1, out_head, kernel_size=3, padding=1),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.ConvTranspose3d(out_head, out_c1, kernel_size=3, padding=1),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.ConvTranspose3d(out_c1, in_c2, kernel_size=3, padding=1),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.ConvTranspose3d(in_c2,in_c1, kernel_size=3, padding=1),\n",
    "                nn.LeakyReLU(),\n",
    "\n",
    "\n",
    "            )\n",
    "            return headnet\n",
    "\n",
    "        def forward(self, inputtensor):\n",
    "            \"\"\"\n",
    "            Forward pass through the network\n",
    "            :param inputtensor: Input tensor\n",
    "            \"\"\"\n",
    "            inputtensor = F.pad(inputtensor, (1, 2, 1, 2, 1, 2))\n",
    "\n",
    "            out = self.en_layer0(inputtensor)\n",
    "            out = self.max_pooling(out)\n",
    "            out = self.en_layer1(out)\n",
    "            out = self.max_pooling(out)\n",
    "            out = self.en_layer2(out)\n",
    "            out = self.max_pooling(out)\n",
    "            out = self.en_layer3(out)\n",
    "            #out = self.max_pooling(out)\n",
    "\n",
    "            #out = self.en_layer4(out)\n",
    "            #out = self.adap_max_pool(out)\n",
    "            #out = out.reshape(out.size(0), -1)  # flatten\n",
    "            out = self.headnet(out)\n",
    "            #out = out.reshape(-1,512,5,5,5)\n",
    "            out = self.de_layer0(out)\n",
    "            out = self.up_sampling(out)\n",
    "            out = self.de_layer1(out)\n",
    "            out = self.up_sampling(out)\n",
    "            out = self.de_layer2(out)\n",
    "            out = self.up_sampling(out)\n",
    "            #out = self.de_layer3(out)\n",
    "            #out = self.up_sampling(out)\n",
    "            out = self.de_layer4(out)\n",
    "            #out = F.normalize(out, p=2, dim=1)\n",
    "\n",
    "            return out\n",
    "\n",
    "    \"\"\"\n",
    "    Custom 3D convnet, nothing fancy\n",
    "    \"\"\"\n",
    "\n",
    "    def setup_norm(self, norm_name : str, norm_kwargs: dict) -> Dict:\n",
    "        norm = {}\n",
    "        if norm_name == AutoEncoder.NORM_BATCHNORM:\n",
    "            norm[\"module\"] = nn.BatchNorm3d\n",
    "        if norm_name == AutoEncoder.NORM_GROUPNORM:\n",
    "            norm[\"module\"] = nn.GroupNorm\n",
    "        norm[\"kwargs\"] = norm_kwargs\n",
    "\n",
    "        return norm\n",
    "\n",
    "\n",
    "    def setup_gem_pooling(self,gem_pooling_p : float) -> Union[None, nn.Module]:\n",
    "        gem_pooling = None\n",
    "        if gem_pooling_p > 0:\n",
    "            from tomotwin.modules.networks.GeneralizedMeanPooling import GeneralizedMeanPooling\n",
    "            gem_pooling = GeneralizedMeanPooling(norm=gem_pooling_p, output_size=(2, 2, 2))\n",
    "        return gem_pooling\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        norm_name: str,\n",
    "        norm_kwargs: Dict = {},\n",
    "        output_channels: int = 128,\n",
    "        dropout: float = 0.5,\n",
    "        gem_pooling_p: float = 0,\n",
    "        repeat_layers=0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        norm = self.setup_norm(norm_name, norm_kwargs)\n",
    "        gem_pooling = self.setup_gem_pooling(gem_pooling_p)\n",
    "\n",
    "\n",
    "        self.model = self.Model(\n",
    "            output_channels=output_channels,\n",
    "            dropout=dropout,\n",
    "            repeat_layers=repeat_layers,\n",
    "            norm=norm,\n",
    "            gem_pooling=gem_pooling\n",
    "        )\n",
    "\n",
    "    def init_weights(self):\n",
    "        def _init_weights(model):\n",
    "            if isinstance(model, nn.Conv3d):\n",
    "                torch.nn.init.kaiming_normal_(model.weight)\n",
    "\n",
    "        self.model.apply(_init_weights)\n",
    "\n",
    "    def get_model(self) -> nn.Module:\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tomotwin.modules.networks.torchmodel import TorchModel\n",
    "\n",
    "class AutoEncoder(TorchModel):\n",
    "\n",
    "    NORM_BATCHNORM = \"BatchNorm\"\n",
    "    NORM_GROUPNORM = \"GroupNorm\"\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        def make_norm(self, norm: Dict, num_channels: int) -> nn.Module:\n",
    "            if norm[\"module\"] == nn.BatchNorm3d:\n",
    "                norm[\"kwargs\"][\"num_features\"] = num_channels\n",
    "                return norm[\"module\"](**norm[\"kwargs\"])\n",
    "            elif norm[\"module\"] == nn.GroupNorm:\n",
    "                norm[\"kwargs\"][\"num_channels\"] = num_channels\n",
    "                return norm[\"module\"](**norm[\"kwargs\"])\n",
    "            else:\n",
    "                raise ValueError(\"Not supported norm\", norm[\"module\"])\n",
    "\n",
    "\n",
    "        def __init__(\n",
    "            self,\n",
    "            output_channels: int,\n",
    "            norm: Dict,\n",
    "            dropout: float = 0.5,\n",
    "            repeat_layers=0,\n",
    "            gem_pooling = None,\n",
    "        ):\n",
    "            super().__init__()\n",
    "            norm_func = self.make_norm(norm, 64)\n",
    "            self.en_layer0 = self._make_conv_layer(1, 64, norm=norm_func)\n",
    "\n",
    "            norm_func = self.make_norm(norm, 128)\n",
    "            self.en_layer1 = self._make_conv_layer(64, 128, norm=norm_func)\n",
    "\n",
    "            norm_func = self.make_norm(norm, 256)\n",
    "            self.en_layer2 = self._make_conv_layer(128, 256, norm=norm_func)\n",
    "\n",
    "            norm_func = self.make_norm(norm, 512)\n",
    "            self.en_layer3 = self._make_conv_layer(256, 512, norm=norm_func)\n",
    "\n",
    "            norm_func = self.make_norm(norm, 1024)\n",
    "            self.en_layer4 = self._make_conv_layer(512, 1024, norm=norm_func)\n",
    "\n",
    "            self.max_pooling = nn.MaxPool3d((2, 2, 2))\n",
    "            if gem_pooling:\n",
    "                self.adap_max_pool = gem_pooling\n",
    "            else:\n",
    "                self.adap_max_pool = nn.AdaptiveAvgPool3d((2, 2, 2))\n",
    "            \n",
    "            self.headnet = self._make_headnet(\n",
    "                2 * 2 * 2 * 1024, 2048, output_channels, dropout=dropout\n",
    "            )\n",
    "\n",
    "            norm_func = self.make_norm(norm, 512)\n",
    "            self.de_layer0 = self._make_conv_layer(1024, 512, norm=norm_func)\n",
    "\n",
    "            norm_func = self.make_norm(norm, 256)\n",
    "            self.de_layer1 = self._make_conv_layer(512, 256, norm=norm_func)\n",
    "\n",
    "            norm_func = self.make_norm(norm, 128)\n",
    "            self.de_layer2 = self._make_conv_layer(256, 128, norm=norm_func)\n",
    "\n",
    "            norm_func = self.make_norm(norm, 64)\n",
    "            self.de_layer3 = self._make_conv_layer(128, 64, norm=norm_func)\n",
    "\n",
    "            #norm_func = self.make_norm(norm, 1)\n",
    "            #self.de_layer4 = self._make_conv_layer(64, 1, norm=norm_func)\n",
    "            self.de_layer4 = nn.Sequential(\n",
    "                nn.Conv3d(64, 1, kernel_size=3, padding=1),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Conv3d(1, 1, kernel_size=3, padding=1),\n",
    "                nn.LeakyReLU(),\n",
    "            )\n",
    "\n",
    "            self.up_sampling = nn.Upsample(scale_factor =2)\n",
    "\n",
    "        @staticmethod\n",
    "        def _make_conv_layer(in_c: int, out_c: int, norm: nn.Module, padding: int = 1, kernel_size: int =3):\n",
    "            conv_layer = nn.Sequential(\n",
    "                nn.Conv3d(in_c, out_c, kernel_size=kernel_size, padding=padding),\n",
    "                norm,\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Conv3d(out_c, out_c, kernel_size=kernel_size, padding=padding),\n",
    "                norm,\n",
    "                nn.LeakyReLU(),\n",
    "            )\n",
    "            return conv_layer\n",
    "\n",
    "        @staticmethod\n",
    "        def _make_headnet(\n",
    "            in_c1: int, out_c1: int, out_head: int, dropout: float\n",
    "        ) -> nn.Sequential:\n",
    "            headnet = nn.Sequential(\n",
    "                nn.Dropout(p=dropout),\n",
    "                nn.Linear(in_c1, out_c1),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(out_c1, out_c1),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(out_c1, out_head),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(out_head,out_c1),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(out_c1,in_c1)\n",
    "            )\n",
    "            return headnet\n",
    "\n",
    "        def forward(self, inputtensor):\n",
    "            \"\"\"\n",
    "            Forward pass through the network\n",
    "            :param inputtensor: Input tensor\n",
    "            \"\"\"\n",
    "            #inputtensor = F.pad(inputtensor, (1, 2, 1, 2, 1, 2))\n",
    "\n",
    "            out = self.en_layer0(inputtensor)\n",
    "            out = self.max_pooling(out)\n",
    "            out = self.en_layer1(out)\n",
    "            out = self.max_pooling(out)\n",
    "            out = self.en_layer2(out)\n",
    "            out = self.max_pooling(out)\n",
    "            out = self.en_layer3(out)\n",
    "            out = self.max_pooling(out)\n",
    "            out = self.en_layer4(out)\n",
    "            #out = self.adap_max_pool(out)\n",
    "            out = out.reshape(out.size(0), -1) \n",
    "            out = self.headnet(out)\n",
    "            out = out.reshape(-1,1024,2,2,2)\n",
    "            out = self.de_layer0(out)\n",
    "            out = self.up_sampling(out)\n",
    "            out = self.de_layer1(out)\n",
    "            out = self.up_sampling(out)\n",
    "            out = self.de_layer2(out)\n",
    "            out = self.up_sampling(out)\n",
    "            out = self.de_layer3(out)\n",
    "            out = self.up_sampling(out)\n",
    "            out = self.de_layer4(out)\n",
    "            #out = F.normalize(out, p=2, dim=1)\n",
    "\n",
    "            return out\n",
    "\n",
    "    \"\"\"\n",
    "    Custom 3D convnet, nothing fancy\n",
    "    \"\"\"\n",
    "\n",
    "    def setup_norm(self, norm_name : str, norm_kwargs: dict) -> Dict:\n",
    "        norm = {}\n",
    "        if norm_name == AutoEncoder.NORM_BATCHNORM:\n",
    "            norm[\"module\"] = nn.BatchNorm3d\n",
    "        if norm_name == AutoEncoder.NORM_GROUPNORM:\n",
    "            norm[\"module\"] = nn.GroupNorm\n",
    "        norm[\"kwargs\"] = norm_kwargs\n",
    "\n",
    "        return norm\n",
    "\n",
    "\n",
    "    def setup_gem_pooling(self,gem_pooling_p : float) -> Union[None, nn.Module]:\n",
    "        gem_pooling = None\n",
    "        if gem_pooling_p > 0:\n",
    "            from tomotwin.modules.networks.GeneralizedMeanPooling import GeneralizedMeanPooling\n",
    "            gem_pooling = GeneralizedMeanPooling(norm=gem_pooling_p, output_size=(2, 2, 2))\n",
    "        return gem_pooling\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        norm_name: str,\n",
    "        norm_kwargs: Dict = {},\n",
    "        output_channels: int = 128,\n",
    "        dropout: float = 0.5,\n",
    "        gem_pooling_p: float = 0,\n",
    "        repeat_layers=0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        norm = self.setup_norm(norm_name, norm_kwargs)\n",
    "        gem_pooling = self.setup_gem_pooling(gem_pooling_p)\n",
    "\n",
    "\n",
    "        self.model = self.Model(\n",
    "            output_channels=output_channels,\n",
    "            dropout=dropout,\n",
    "            repeat_layers=repeat_layers,\n",
    "            norm=norm,\n",
    "            gem_pooling=gem_pooling\n",
    "        )\n",
    "\n",
    "    def init_weights(self):\n",
    "        def _init_weights(model):\n",
    "            if isinstance(model, nn.Conv3d):\n",
    "                torch.nn.init.kaiming_normal_(model.weight)\n",
    "\n",
    "        self.model.apply(_init_weights)\n",
    "\n",
    "    def get_model(self) -> nn.Module:\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_name = \"GroupNorm\"\n",
    "norm_kwargs = {\"num_groups\": 64,\n",
    "        \"num_channels\": 1024}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = AutoEncoder(norm_name,norm_kwargs,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = M.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 64, 40, 40, 40])\n",
      "torch.Size([12, 128, 10, 10, 10])\n",
      "torch.Size([12, 512, 5, 5, 5])\n",
      "torch.Size([12, 512, 5, 5, 5])\n",
      "torch.Size([12, 256, 10, 10, 10])\n",
      "torch.Size([12, 128, 20, 20, 20])\n",
      "torch.Size([12, 64, 20, 20, 20])\n",
      "torch.Size([12, 64, 40, 40, 40])\n",
      "torch.Size([12, 1, 40, 40, 40])\n"
     ]
    }
   ],
   "source": [
    "out = Model(torch.rand(12,1,37,37,37))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 1, 40, 40, 40])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tomotwin.modules.training.mrctriplethandler import MRCTripletHandler\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MRCVolumeDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.file_paths = self._get_file_paths()\n",
    "        self.reader = MRCTripletHandler()\n",
    "\n",
    "    def _get_file_paths(self):\n",
    "        file_paths = []\n",
    "        for round_dir in os.listdir(self.root_dir):\n",
    "            round_path = os.path.join(self.root_dir, round_dir)\n",
    "            if os.path.isdir(round_path):\n",
    "                for tomo_dir in os.listdir(round_path):\n",
    "                    tomo_path = os.path.join(round_path, tomo_dir)\n",
    "                    if os.path.isdir(tomo_path):\n",
    "                        mrc_files = [f for f in os.listdir(tomo_path) if f.endswith('.mrc')]\n",
    "                        for mrc_file in mrc_files:\n",
    "                            file_paths.append(os.path.join(tomo_path, mrc_file))\n",
    "        return file_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mrc_path = self.file_paths[idx]\n",
    "        volume = self.reader.read_mrc_and_norm(mrc_path)\n",
    "\n",
    "        return {'input': volume, 'target': volume}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "root_dir = '/home/yousef.metwally/projects/data/tomotwin_training_data/validation'\n",
    "\n",
    "dataset = MRCVolumeDataset(root_dir)\n",
    "batch_size = 32\n",
    "shuffle = True  \n",
    "num_workers = 4  \n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def loss_function(recon_x, x):\n",
    "    mse_loss = F.mse_loss(recon_x, x)\n",
    "    return mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train_autoencoder(model, data_loader, optimizer, num_epochs=10, device='cuda'):\n",
    "    writer = SummaryWriter('/home/yousef.metwally/projects/AutoEncoder')\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        with tqdm(data_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as progress_bar:\n",
    "            for batch_idx, data in enumerate(progress_bar):\n",
    "                input_data = data['input'].to(device)\n",
    "                #if batch_idx > 500:\n",
    "                 #   print(input_data.shape)\n",
    "                input_data = input_data.reshape(-1,1,37,37,37)\n",
    "                target_data = data['target'].to(device)\n",
    "                target_data = F.pad(target_data, (1, 2, 1, 2, 1, 2))\n",
    "                target_data = target_data.reshape(-1,1,40,40,40)\n",
    "                optimizer.zero_grad()\n",
    "                recon_data = model(input_data)\n",
    "                loss = loss_function(recon_data, target_data)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                avg_loss = total_loss / (batch_idx + 1)\n",
    "                writer.add_scalar('Loss/train', avg_loss, epoch * len(data_loader) + batch_idx)\n",
    "\n",
    "                \n",
    "                progress_bar.set_postfix(loss=avg_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Avg. Loss: {avg_loss:.4f}\")\n",
    "        torch.save(model.state_dict(), f\"/home/yousef.metwally/projects/AutoEncoder/weights/run1/model_weights_epoch_{epoch+1}.pt\")\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "num_epochs = 200\n",
    "M = AutoEncoder(norm_name,norm_kwargs,32)\n",
    "model = M.get_model()\n",
    "model = nn.DataParallel(model)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_autoencoder(model, data_loader, optimizer, num_epochs, device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class testDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.file_paths = self._get_file_paths()\n",
    "        self.reader = MRCTripletHandler()\n",
    "\n",
    "    def _get_file_paths(self):\n",
    "        file_paths = [os.path.join(self.root_dir, f) for f in os.listdir(self.root_dir) if f.endswith('.mrc')]\n",
    "        return file_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mrc_path = self.file_paths[idx]\n",
    "        volume = self.reader.read_mrc_and_norm(mrc_path)\n",
    "        return {'input': volume, 'target': volume}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '/home/yousef.metwally/projects/data/tomotwin_training_data/test'\n",
    "test_dataset = testDataset(test_dir)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "model = UNet3D(1,1)\n",
    "model = nn.DataParallel(model)\n",
    "checkpoint_path = '/home/yousef.metwally/projects/UNet/weights/model_weights_epoch_16.pt'\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint)\n",
    "device = 'cuda'\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2/2 [00:01<00:00,  1.20batch/s, loss=1.49e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.2851032178912064e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate (model, data_loader):\n",
    "    total_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        with tqdm(data_loader, desc=\"Evaluating\", unit=\"batch\") as progress_bar:\n",
    "            for batch_idx, data in enumerate(progress_bar):\n",
    "                input_data = data['input'].to(device)\n",
    "                input_data = input_data.reshape(-1, 1, 37, 37, 37)\n",
    "                target_data = data['target'].to(device)\n",
    "                target_data = target_data.reshape(-1, 1, 37, 37, 37)\n",
    "                recon_data = model(input_data)\n",
    "                loss = loss_function(recon_data, target_data)\n",
    "                total_loss += loss.item()\n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f\"Validation Loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_data = batch['input'].unsqueeze(1)\n",
    "            target_data = batch['target'].unsqueeze(1)\n",
    "            output = model(input_data)\n",
    "            predictions.append(output.squeeze(1).cpu().numpy())\n",
    "            targets.append(target_data.squeeze(1).cpu().numpy())\n",
    "    return predictions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mrcfile\n",
    "def save_as_mrc(data, filename):\n",
    "    with mrcfile.new(filename, overwrite=True) as mrc:\n",
    "        mrc.set_data(data.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, targets = predict(model, test_loader)\n",
    "save_dir = '/home/yousef.metwally/projects/UNet/output'\n",
    "def save_ouput (out_dir: str, predictions, targets):\n",
    "    for i, (pred, target) in enumerate(zip(predictions, targets)):\n",
    "        pred_filename = os.path.join(out_dir, f'prediction_{i:03d}.mrc')\n",
    "        target_filename = os.path.join(out_dir, f'target_{i:03d}.mrc')\n",
    "        save_as_mrc(pred, pred_filename)\n",
    "        save_as_mrc(target, target_filename)\n",
    "        print(f'Saved prediction to {pred_filename} and target to {target_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "TensorBoard 2.16.2 at http://gtxr3.srv-local.mpi-dortmund.mpg.de:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir='/home/yousef.metwally/projects/UNet/' --bind_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Model:\n\tsize mismatch for de_layer0.0.weight: copying a param with shape torch.Size([512, 256, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 3, 3, 3]).\n\tsize mismatch for de_layer1.0.weight: copying a param with shape torch.Size([256, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 3, 3, 3]).\n\tsize mismatch for de_layer2.0.weight: copying a param with shape torch.Size([128, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 3, 3, 3]).\n\tsize mismatch for de_layer4.0.weight: copying a param with shape torch.Size([64, 1, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([1, 64, 3, 3, 3]).\n\tsize mismatch for de_layer4.2.weight: copying a param with shape torch.Size([1, 1, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 1, 1]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(checkpoint_path)\n\u001b[1;32m      8\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m {k\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodule.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m): v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m checkpoint\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tomotwin/lib/python3.10/site-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Model:\n\tsize mismatch for de_layer0.0.weight: copying a param with shape torch.Size([512, 256, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 3, 3, 3]).\n\tsize mismatch for de_layer1.0.weight: copying a param with shape torch.Size([256, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 3, 3, 3]).\n\tsize mismatch for de_layer2.0.weight: copying a param with shape torch.Size([128, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 3, 3, 3]).\n\tsize mismatch for de_layer4.0.weight: copying a param with shape torch.Size([64, 1, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([1, 64, 3, 3, 3]).\n\tsize mismatch for de_layer4.2.weight: copying a param with shape torch.Size([1, 1, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 1, 1])."
     ]
    }
   ],
   "source": [
    "reader = MRCTripletHandler()\n",
    "v_path = '/home/yousef.metwally/projects/data/tomotwin_training_data/test/round01_t08_0XXX_000.mrc'\n",
    "volume = reader.read_mrc_and_norm(v_path)\n",
    "M = AutoEncoder(norm_name,norm_kwargs,32)\n",
    "model = M.get_model()\n",
    "checkpoint_path = '/home/yousef.metwally/projects/AutoEncoder/weights/run1/model_weights_epoch_200.pt'\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "state_dict = {k.replace('module.', ''): v for k, v in checkpoint.items()}\n",
    "model.load_state_dict(state_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = os.listdir(\"/home/yousef.metwally/projects/AutoEncoder/weights/run1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_weights_epoch_197.pt',\n",
       " 'model_weights_epoch_198.pt',\n",
       " 'model_weights_epoch_199.pt',\n",
       " 'model_weights_epoch_200.pt']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Down, self).__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool3d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super(Up, self).__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose3d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # Ensure the shapes match for concatenation\n",
    "        diffZ = x2.size(2) - x1.size(2)\n",
    "        diffY = x2.size(3) - x1.size(3)\n",
    "        diffX = x2.size(4) - x1.size(4)\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2,\n",
    "                        diffZ // 2, diffZ - diffZ // 2])\n",
    "        \n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet3D, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 512)\n",
    "        self.up1 = Up(1024, 256, bilinear)\n",
    "        self.up2 = Up(512, 128, bilinear)\n",
    "        self.up3 = Up(256, 64, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d) or isinstance(m, nn.ConvTranspose3d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from typing import Dict, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tomotwin.modules.networks.torchmodel import TorchModel\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from tomotwin.modules.training.mrctriplethandler import MRCTripletHandler\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Down, self).__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool3d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super(Up, self).__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose3d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffZ = x2.size(2) - x1.size(2)\n",
    "        diffY = x2.size(3) - x1.size(3)\n",
    "        diffX = x2.size(4) - x1.size(4)\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2,\n",
    "                        diffZ // 2, diffZ - diffZ // 2])\n",
    "        \n",
    "        #x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x1)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, n_channels, out_channels, bilinear=True):\n",
    "        super(UNet3D, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024)\n",
    "        self.down5 = Down(1024,1024)\n",
    "        self.up1 = Up(1024, 512, bilinear)\n",
    "        self.up2 = Up(512, 256, bilinear)\n",
    "        self.up3 = Up(256, 128, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.up5 = Up(64,64)\n",
    "        self.outc = OutConv(64, out_channels)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x6 = self.down5(x5)\n",
    "        x = self.up1(x6, x5)\n",
    "        x = self.up2(x, x4)\n",
    "        x = self.up3(x, x3)\n",
    "        x = self.up4(x, x2)\n",
    "        x = self.up5(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d) or isinstance(m, nn.ConvTranspose3d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet3D(1,1)\n",
    "out = model(torch.rand(12,1,37,37,37))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def conv_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv3d(inp, oup, kernel_size=3, stride=stride, padding=(1,1,1), bias=False),\n",
    "        nn.BatchNorm3d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv3d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm3d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "\n",
    "        hidden_dim = round(inp * expand_ratio)\n",
    "        self.use_res_connect = self.stride == (1,1,1) and inp == oup\n",
    "\n",
    "        if expand_ratio == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv3d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm3d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv3d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm3d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv3d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm3d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv3d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm3d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv3d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm3d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=1000, sample_size=224, width_mult=1.):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "        interverted_residual_setting = [\n",
    "            # t, c, n, s\n",
    "            [1,  32, 1, (1,1,1)],\n",
    "            [6,  64, 2, (2,2,2)],\n",
    "            [6,  128, 3, (2,2,2)],\n",
    "            [6,  256, 4, (2,2,2)],\n",
    "            [6,  512, 3, (1,1,1)],\n",
    "            [6, 1024, 3, (2,2,2)],\n",
    "            [6, 2048, 1, (1,1,1)],\n",
    "        ]\n",
    "\n",
    "        # building first layer\n",
    "        assert sample_size % 16 == 0.\n",
    "        input_channel = int(input_channel * width_mult)\n",
    "        self.last_channel = int(last_channel * width_mult) if width_mult > 1.0 else last_channel\n",
    "        self.features = [conv_bn(1, input_channel, (1,2,2))]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in interverted_residual_setting:\n",
    "            output_channel = int(c * width_mult)\n",
    "            for i in range(n):\n",
    "                stride = s if i == 0 else (1,1,1)\n",
    "                self.features.append(block(input_channel, output_channel, stride, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        self.features.append(conv_1x1x1_bn(input_channel, self.last_channel))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*self.features)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.features):\n",
    "            x = layer(x)\n",
    "           # print(f\"Output after layer {i}: {x.shape}\")\n",
    "        x = F.avg_pool3d(x, x.data.size()[-3:])\n",
    "       # print(f\"Output after avg_pool3d: {x.shape}\")\n",
    "        x = x.view(x.size(0), -1)\n",
    "       # print(f\"Output after view: {x.shape}\")\n",
    "        #x = self.classifier(x)\n",
    "        #print(f\"Output after classifier: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.kernel_size[2] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "class MobileNetV2Autoencoder(nn.Module):\n",
    "    def __init__(self, num_classes=1000, sample_size=32, width_mult=1.):\n",
    "        super(MobileNetV2Autoencoder, self).__init__()\n",
    "        self.encoder = MobileNetV2(num_classes=num_classes, sample_size=sample_size, width_mult=width_mult)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose3d(2048, 320, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(320),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.ConvTranspose3d(320, 160, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(160),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.ConvTranspose3d(160, 96, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(96),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.ConvTranspose3d(96, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.ConvTranspose3d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.ConvTranspose3d(32, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()  # Assuming the output needs to be normalized\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "       # print(\"Input shape:\", x.shape)\n",
    "        for i, layer in enumerate(self.encoder.features):\n",
    "            x = layer(x)\n",
    "         #   print(f\"Encoder output after layer {i}: {x.shape}\")\n",
    "        \n",
    "        x = F.avg_pool3d(x, x.data.size()[-3:])\n",
    "       # print(f\"Output after avg_pool3d: {x.shape}\")\n",
    "        x = x.view(x.size(0), -1, 1, 1, 1)\n",
    "      #  print(f\"Output after view: {x.shape}\")\n",
    "\n",
    "        for i, layer in enumerate(self.decoder):\n",
    "            x = layer(x)\n",
    "           # print(f\"Decoder output after layer {i}: {x.shape}\")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import random\n",
    "from typing import Dict, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tomotwin.modules.networks.torchmodel import TorchModel\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from tomotwin.modules.training.mrctriplethandler import MRCTripletHandler\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import mrcfile\n",
    "class MRCVolumeDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.file_paths = self._get_file_paths()\n",
    "\n",
    "    def read_and_normalize_mrc(self, file_path):\n",
    "        with mrcfile.open(file_path, permissive=True) as mrc:\n",
    "            data = mrc.data.astype(np.float32)\n",
    "            min_val = np.min(data)\n",
    "            max_val = np.max(data)\n",
    "            normalized_data = (data - min_val) / (max_val - min_val)\n",
    "        return normalized_data\n",
    "\n",
    "    def _get_file_paths(self):\n",
    "        file_paths = []\n",
    "        for round_dir in os.listdir(self.root_dir):\n",
    "            round_path = os.path.join(self.root_dir, round_dir)\n",
    "            if os.path.isdir(round_path):\n",
    "                for tomo_dir in os.listdir(round_path):\n",
    "                    tomo_path = os.path.join(round_path, tomo_dir)\n",
    "                    if os.path.isdir(tomo_path):\n",
    "                        mrc_files = [f for f in os.listdir(tomo_path) if f.endswith('.mrc')]\n",
    "                        for mrc_file in mrc_files:\n",
    "                            file_paths.append(os.path.join(tomo_path, mrc_file))\n",
    "                                                                                     \n",
    "        return file_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mrc_path = self.file_paths[idx]\n",
    "        volume = self.read_and_normalize_mrc(mrc_path)\n",
    "        volume = volume[2:-3,2:-3,2:-3]\n",
    "\n",
    "        return {'input': volume, 'target': volume}\n",
    "\n",
    "    \n",
    "\n",
    "def loss_function(recon_x, x):\n",
    "    mse_loss = F.mse_loss(recon_x, x)\n",
    "    return mse_loss\n",
    "\n",
    "\n",
    "def train_autoencoder(model, data_loader, val_loader, optimizer, scheduler, logging, num_epochs=10, device='cuda', patience=10):\n",
    "    writer = SummaryWriter(logging)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    if not os.path.exists(f\"{logging}/weights\"):\n",
    "        os.makedirs(f\"{logging}/weights\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        with tqdm(data_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as progress_bar:\n",
    "            for batch_idx, data in enumerate(progress_bar):\n",
    "                input_data = data['input'].to(device)\n",
    "                input_data = input_data.unsqueeze(1)\n",
    "                target_data = data['target'].to(device)\n",
    "                target_data = target_data.unsqueeze(1)\n",
    "                #target_data = F.pad(target_data, (1, 2, 1, 2, 1, 2))  \n",
    "                optimizer.zero_grad()\n",
    "                recon_data = model(input_data)\n",
    "                loss = loss_function(recon_data, target_data)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                avg_loss = total_loss / (batch_idx + 1)\n",
    "                writer.add_scalar('Loss/train', avg_loss, epoch * len(data_loader) + batch_idx)\n",
    "                progress_bar.set_postfix(loss=avg_loss)\n",
    "                \n",
    "        val_loss = validate_autoencoder(model,val_loader,writer, epoch+1,num_epochs)\n",
    "        scheduler.step(val_loss)\n",
    "        if val_loss < best_loss:\n",
    "            print (f'{val_loss} < {best_loss}, epoch: {epoch+1} ')\n",
    "            torch.save(model.state_dict(), f\"{logging}/weights/model_weights_epoch_{epoch+1}.pt\")\n",
    "            best_loss = val_loss\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= patience:\n",
    "                print(f\"Validation loss did not improve for {patience} epochs. Early stopping...\")\n",
    "                break\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Avg. Loss: {avg_loss}, Val. Loss: {val_loss}\")\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "def validate_autoencoder(model, data_loader, writer, epoch,num_epochs, device='cuda'):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(data_loader, desc=f\"Validation Epoch {epoch}/{num_epochs}\", unit=\"batch\") as progress_bar:\n",
    "            for batch_idx, data in enumerate(progress_bar):\n",
    "                input_data = data['input'].to(device)\n",
    "                input_data = input_data.unsqueeze(1)\n",
    "                target_data = data['target'].to(device)\n",
    "                target_data = target_data.unsqueeze(1)\n",
    "                #target_data = F.pad(target_data, (1, 2, 1, 2, 1, 2))  \n",
    "                recon_data = model(input_data)\n",
    "                loss = loss_function(recon_data, target_data)\n",
    "                total_loss += loss.item()\n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    writer.add_scalar('Loss/val', avg_loss, epoch)\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000: 100%|██████████| 1682/1682 [04:40<00:00,  6.00batch/s, loss=0.0134]\n",
      "Validation Epoch 1/1000: 100%|██████████| 281/281 [00:27<00:00, 10.40batch/s, loss=0.0123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012403838492240558 < inf, epoch: 1 \n",
      "Epoch 1/1000, Avg. Loss: 0.013419953164368796, Val. Loss: 0.012403838492240558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000: 100%|██████████| 1682/1682 [04:40<00:00,  5.99batch/s, loss=0.0127]\n",
      "Validation Epoch 2/1000: 100%|██████████| 281/281 [00:28<00:00,  9.99batch/s, loss=0.0114]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012360871772802174 < 0.012403838492240558, epoch: 2 \n",
      "Epoch 2/1000, Avg. Loss: 0.012724148594042315, Val. Loss: 0.012360871772802174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000: 100%|██████████| 1682/1682 [04:43<00:00,  5.94batch/s, loss=0.0127]\n",
      "Validation Epoch 3/1000: 100%|██████████| 281/281 [00:27<00:00, 10.17batch/s, loss=0.0116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000, Avg. Loss: 0.01267022962928899, Val. Loss: 0.0123866999926198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000: 100%|██████████| 1682/1682 [04:33<00:00,  6.14batch/s, loss=0.0126]\n",
      "Validation Epoch 4/1000: 100%|██████████| 281/281 [00:25<00:00, 10.92batch/s, loss=0.0128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01235434081856148 < 0.012360871772802174, epoch: 4 \n",
      "Epoch 4/1000, Avg. Loss: 0.012608975928950076, Val. Loss: 0.01235434081856148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000: 100%|██████████| 1682/1682 [04:34<00:00,  6.13batch/s, loss=0.0126]\n",
      "Validation Epoch 5/1000: 100%|██████████| 281/281 [00:27<00:00, 10.35batch/s, loss=0.0124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000, Avg. Loss: 0.012606804372629817, Val. Loss: 0.012359671490233776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000: 100%|██████████| 1682/1682 [04:42<00:00,  5.95batch/s, loss=0.0126]\n",
      "Validation Epoch 6/1000: 100%|██████████| 281/281 [00:27<00:00, 10.13batch/s, loss=0.0127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000, Avg. Loss: 0.012598589330127597, Val. Loss: 0.012390864704772034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000: 100%|██████████| 1682/1682 [04:40<00:00,  6.00batch/s, loss=0.0126]\n",
      "Validation Epoch 7/1000: 100%|██████████| 281/281 [00:27<00:00, 10.17batch/s, loss=0.0128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000, Avg. Loss: 0.01257380468307969, Val. Loss: 0.012382255002087334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000: 100%|██████████| 1682/1682 [04:38<00:00,  6.03batch/s, loss=0.0126]\n",
      "Validation Epoch 8/1000: 100%|██████████| 281/281 [00:27<00:00, 10.36batch/s, loss=0.0122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01235201530277835 < 0.01235434081856148, epoch: 8 \n",
      "Epoch 8/1000, Avg. Loss: 0.012568638466218848, Val. Loss: 0.01235201530277835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000: 100%|██████████| 1682/1682 [04:40<00:00,  6.00batch/s, loss=0.0126]\n",
      "Validation Epoch 9/1000: 100%|██████████| 281/281 [00:30<00:00,  9.29batch/s, loss=0.0129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000, Avg. Loss: 0.01256760964491244, Val. Loss: 0.013267714894804241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000: 100%|██████████| 1682/1682 [04:33<00:00,  6.14batch/s, loss=0.0126]\n",
      "Validation Epoch 10/1000: 100%|██████████| 281/281 [00:28<00:00,  9.95batch/s, loss=0.0127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012341636388804565 < 0.01235201530277835, epoch: 10 \n",
      "Epoch 10/1000, Avg. Loss: 0.012629973882638743, Val. Loss: 0.012341636388804565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000: 100%|██████████| 1682/1682 [04:38<00:00,  6.03batch/s, loss=0.0126]\n",
      "Validation Epoch 11/1000: 100%|██████████| 281/281 [00:27<00:00, 10.33batch/s, loss=0.0123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012336004336507082 < 0.012341636388804565, epoch: 11 \n",
      "Epoch 11/1000, Avg. Loss: 0.01257794929223133, Val. Loss: 0.012336004336507082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000: 100%|██████████| 1682/1682 [04:38<00:00,  6.05batch/s, loss=0.0126]\n",
      "Validation Epoch 12/1000: 100%|██████████| 281/281 [00:28<00:00, 10.02batch/s, loss=0.0125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012317320756163461 < 0.012336004336507082, epoch: 12 \n",
      "Epoch 12/1000, Avg. Loss: 0.012552832642528057, Val. Loss: 0.012317320756163461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000: 100%|██████████| 1682/1682 [04:36<00:00,  6.08batch/s, loss=0.0125]\n",
      "Validation Epoch 13/1000: 100%|██████████| 281/281 [00:26<00:00, 10.48batch/s, loss=0.0117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000, Avg. Loss: 0.012549906224344595, Val. Loss: 0.012327491978574478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000: 100%|██████████| 1682/1682 [04:43<00:00,  5.93batch/s, loss=0.0125]\n",
      "Validation Epoch 14/1000: 100%|██████████| 281/281 [00:29<00:00,  9.38batch/s, loss=0.0124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000, Avg. Loss: 0.012546363328755186, Val. Loss: 0.012781323758997952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000: 100%|██████████| 1682/1682 [04:42<00:00,  5.95batch/s, loss=0.0126]\n",
      "Validation Epoch 15/1000: 100%|██████████| 281/281 [00:28<00:00,  9.69batch/s, loss=0.0129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012315323844790671 < 0.012317320756163461, epoch: 15 \n",
      "Epoch 15/1000, Avg. Loss: 0.01256249902114343, Val. Loss: 0.012315323844790671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/1000: 100%|██████████| 1682/1682 [04:40<00:00,  6.00batch/s, loss=0.0125]\n",
      "Validation Epoch 16/1000: 100%|██████████| 281/281 [00:25<00:00, 10.94batch/s, loss=0.0126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012292618193792916 < 0.012315323844790671, epoch: 16 \n",
      "Epoch 16/1000, Avg. Loss: 0.012542468836745879, Val. Loss: 0.012292618193792916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/1000: 100%|██████████| 1682/1682 [04:38<00:00,  6.03batch/s, loss=0.0125]\n",
      "Validation Epoch 17/1000: 100%|██████████| 281/281 [00:26<00:00, 10.73batch/s, loss=0.0124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/1000, Avg. Loss: 0.012528378019213464, Val. Loss: 0.012298139319274561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000: 100%|██████████| 1682/1682 [04:37<00:00,  6.07batch/s, loss=0.0126]\n",
      "Validation Epoch 18/1000: 100%|██████████| 281/281 [00:27<00:00, 10.33batch/s, loss=0.0128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000, Avg. Loss: 0.012564352456608722, Val. Loss: 0.0123039803457748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/1000: 100%|██████████| 1682/1682 [04:41<00:00,  5.98batch/s, loss=0.0125]\n",
      "Validation Epoch 19/1000: 100%|██████████| 281/281 [00:28<00:00,  9.90batch/s, loss=0.0128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/1000, Avg. Loss: 0.012523948276408823, Val. Loss: 0.012293010227567784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000: 100%|██████████| 1682/1682 [04:37<00:00,  6.05batch/s, loss=0.0126]\n",
      "Validation Epoch 20/1000: 100%|██████████| 281/281 [00:28<00:00,  9.91batch/s, loss=0.0122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000, Avg. Loss: 0.012552360497419495, Val. Loss: 0.01231345408380668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/1000: 100%|██████████| 1682/1682 [04:38<00:00,  6.04batch/s, loss=0.0125]\n",
      "Validation Epoch 21/1000: 100%|██████████| 281/281 [00:26<00:00, 10.48batch/s, loss=0.0116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012290422025789569 < 0.012292618193792916, epoch: 21 \n",
      "Epoch 21/1000, Avg. Loss: 0.012530535577456429, Val. Loss: 0.012290422025789569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000: 100%|██████████| 1682/1682 [04:36<00:00,  6.07batch/s, loss=0.0125]\n",
      "Validation Epoch 22/1000: 100%|██████████| 281/281 [00:26<00:00, 10.41batch/s, loss=0.0118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000, Avg. Loss: 0.012515806693774199, Val. Loss: 0.012293305804827036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/1000: 100%|██████████| 1682/1682 [04:37<00:00,  6.07batch/s, loss=0.0125]\n",
      "Validation Epoch 23/1000: 100%|██████████| 281/281 [00:26<00:00, 10.78batch/s, loss=0.0122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01228850754475042 < 0.012290422025789569, epoch: 23 \n",
      "Epoch 23/1000, Avg. Loss: 0.01251049694969506, Val. Loss: 0.01228850754475042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/1000: 100%|██████████| 1682/1682 [04:32<00:00,  6.17batch/s, loss=0.0125]\n",
      "Validation Epoch 24/1000: 100%|██████████| 281/281 [00:26<00:00, 10.53batch/s, loss=0.0126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012282330178228138 < 0.01228850754475042, epoch: 24 \n",
      "Epoch 24/1000, Avg. Loss: 0.01250447417907284, Val. Loss: 0.012282330178228138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000: 100%|██████████| 1682/1682 [04:32<00:00,  6.16batch/s, loss=0.0125]\n",
      "Validation Epoch 25/1000: 100%|██████████| 281/281 [00:26<00:00, 10.75batch/s, loss=0.0123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012267800926287192 < 0.012282330178228138, epoch: 25 \n",
      "Epoch 25/1000, Avg. Loss: 0.012504460281215156, Val. Loss: 0.012267800926287192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.19batch/s, loss=0.0125]\n",
      "Validation Epoch 26/1000: 100%|██████████| 281/281 [00:25<00:00, 10.87batch/s, loss=0.0117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012265985659273919 < 0.012267800926287192, epoch: 26 \n",
      "Epoch 26/1000, Avg. Loss: 0.0124951353231301, Val. Loss: 0.012265985659273919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/1000: 100%|██████████| 1682/1682 [04:37<00:00,  6.05batch/s, loss=0.0125]\n",
      "Validation Epoch 27/1000: 100%|██████████| 281/281 [00:27<00:00, 10.18batch/s, loss=0.0122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/1000, Avg. Loss: 0.012505322741289201, Val. Loss: 0.012275534937178112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/1000: 100%|██████████| 1682/1682 [04:36<00:00,  6.08batch/s, loss=0.0125]\n",
      "Validation Epoch 28/1000: 100%|██████████| 281/281 [00:27<00:00, 10.33batch/s, loss=0.0125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/1000, Avg. Loss: 0.012494600801003319, Val. Loss: 0.01228830031056315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/1000: 100%|██████████| 1682/1682 [04:37<00:00,  6.07batch/s, loss=0.0125]\n",
      "Validation Epoch 29/1000: 100%|██████████| 281/281 [00:27<00:00, 10.22batch/s, loss=0.0124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01225878418923697 < 0.012265985659273919, epoch: 29 \n",
      "Epoch 29/1000, Avg. Loss: 0.012486987839166106, Val. Loss: 0.01225878418923697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/1000: 100%|██████████| 1682/1682 [04:39<00:00,  6.01batch/s, loss=0.0125]\n",
      "Validation Epoch 30/1000: 100%|██████████| 281/281 [00:25<00:00, 10.87batch/s, loss=0.0122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/1000, Avg. Loss: 0.012482472581901853, Val. Loss: 0.012282517622639277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/1000: 100%|██████████| 1682/1682 [04:30<00:00,  6.21batch/s, loss=0.0125]\n",
      "Validation Epoch 31/1000: 100%|██████████| 281/281 [00:25<00:00, 10.89batch/s, loss=0.0127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/1000, Avg. Loss: 0.012480444382260444, Val. Loss: 0.012267522563581153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.19batch/s, loss=0.0125]\n",
      "Validation Epoch 32/1000: 100%|██████████| 281/281 [00:26<00:00, 10.64batch/s, loss=0.0126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012250383944645046 < 0.01225878418923697, epoch: 32 \n",
      "Epoch 32/1000, Avg. Loss: 0.012478634753285527, Val. Loss: 0.012250383944645046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/1000: 100%|██████████| 1682/1682 [04:30<00:00,  6.21batch/s, loss=0.0125]\n",
      "Validation Epoch 33/1000: 100%|██████████| 281/281 [00:26<00:00, 10.78batch/s, loss=0.0128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/1000, Avg. Loss: 0.012474935871131462, Val. Loss: 0.012263539674761457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.19batch/s, loss=0.0125]\n",
      "Validation Epoch 34/1000: 100%|██████████| 281/281 [00:25<00:00, 10.88batch/s, loss=0.0123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012245708161772782 < 0.012250383944645046, epoch: 34 \n",
      "Epoch 34/1000, Avg. Loss: 0.012474915193223288, Val. Loss: 0.012245708161772782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.20batch/s, loss=0.0125]\n",
      "Validation Epoch 35/1000: 100%|██████████| 281/281 [00:25<00:00, 10.85batch/s, loss=0.0124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/1000, Avg. Loss: 0.01250464963814184, Val. Loss: 0.01225349381632334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.19batch/s, loss=0.0125]\n",
      "Validation Epoch 36/1000: 100%|██████████| 281/281 [00:25<00:00, 10.90batch/s, loss=0.0126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012238296481293504 < 0.012245708161772782, epoch: 36 \n",
      "Epoch 36/1000, Avg. Loss: 0.012471850735537362, Val. Loss: 0.012238296481293504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.19batch/s, loss=0.0125]\n",
      "Validation Epoch 37/1000: 100%|██████████| 281/281 [00:25<00:00, 10.88batch/s, loss=0.0118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/1000, Avg. Loss: 0.012479541158274826, Val. Loss: 0.012248791638244726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.20batch/s, loss=0.0125]\n",
      "Validation Epoch 38/1000: 100%|██████████| 281/281 [00:26<00:00, 10.61batch/s, loss=0.0116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/1000, Avg. Loss: 0.012470336796496591, Val. Loss: 0.012248143344931968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.21batch/s, loss=0.0125]\n",
      "Validation Epoch 39/1000: 100%|██████████| 281/281 [00:26<00:00, 10.70batch/s, loss=0.0115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012233959759496073 < 0.012238296481293504, epoch: 39 \n",
      "Epoch 39/1000, Avg. Loss: 0.012466550481314112, Val. Loss: 0.012233959759496073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.20batch/s, loss=0.0125]\n",
      "Validation Epoch 40/1000: 100%|██████████| 281/281 [00:25<00:00, 10.83batch/s, loss=0.012] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/1000, Avg. Loss: 0.012465715532353885, Val. Loss: 0.012242226069295958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.20batch/s, loss=0.0125]\n",
      "Validation Epoch 41/1000: 100%|██████████| 281/281 [00:25<00:00, 10.88batch/s, loss=0.0128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/1000, Avg. Loss: 0.012462104500163962, Val. Loss: 0.012251047526893879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.20batch/s, loss=0.0125]\n",
      "Validation Epoch 42/1000: 100%|██████████| 281/281 [00:27<00:00, 10.32batch/s, loss=0.0118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012223453751476846 < 0.012233959759496073, epoch: 42 \n",
      "Epoch 42/1000, Avg. Loss: 0.012460000962506223, Val. Loss: 0.012223453751476846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.20batch/s, loss=0.0125]\n",
      "Validation Epoch 43/1000: 100%|██████████| 281/281 [00:26<00:00, 10.74batch/s, loss=0.0117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/1000, Avg. Loss: 0.012455436331856726, Val. Loss: 0.01224585263277288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.21batch/s, loss=0.0125]\n",
      "Validation Epoch 44/1000: 100%|██████████| 281/281 [00:25<00:00, 10.84batch/s, loss=0.012] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/1000, Avg. Loss: 0.01245517075874629, Val. Loss: 0.012228671196198549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.20batch/s, loss=0.0125]\n",
      "Validation Epoch 45/1000: 100%|██████████| 281/281 [00:25<00:00, 10.86batch/s, loss=0.0116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/1000, Avg. Loss: 0.012450681130017523, Val. Loss: 0.012225683089146835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/1000: 100%|██████████| 1682/1682 [04:30<00:00,  6.21batch/s, loss=0.0124]\n",
      "Validation Epoch 46/1000: 100%|██████████| 281/281 [00:26<00:00, 10.47batch/s, loss=0.0123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/1000, Avg. Loss: 0.012447136530694724, Val. Loss: 0.012271886714651278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.20batch/s, loss=0.0125]\n",
      "Validation Epoch 47/1000: 100%|██████████| 281/281 [00:25<00:00, 10.83batch/s, loss=0.0126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/1000, Avg. Loss: 0.01245223550296496, Val. Loss: 0.012227872270573713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.20batch/s, loss=0.0124]\n",
      "Validation Epoch 48/1000: 100%|██████████| 281/281 [00:25<00:00, 10.90batch/s, loss=0.0122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012210643508892467 < 0.012223453751476846, epoch: 48 \n",
      "Epoch 48/1000, Avg. Loss: 0.01244552106420453, Val. Loss: 0.012210643508892467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.20batch/s, loss=0.0124]\n",
      "Validation Epoch 49/1000: 100%|██████████| 281/281 [00:25<00:00, 10.90batch/s, loss=0.013] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/1000, Avg. Loss: 0.012442945716426432, Val. Loss: 0.012236524853868629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.19batch/s, loss=0.0124]\n",
      "Validation Epoch 50/1000: 100%|██████████| 281/281 [00:27<00:00, 10.40batch/s, loss=0.0125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/1000, Avg. Loss: 0.012439376129198479, Val. Loss: 0.0122237100905583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.19batch/s, loss=0.0124]\n",
      "Validation Epoch 51/1000: 100%|██████████| 281/281 [00:26<00:00, 10.79batch/s, loss=0.012] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/1000, Avg. Loss: 0.012436550689748223, Val. Loss: 0.012215736332047877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.19batch/s, loss=0.0124]\n",
      "Validation Epoch 52/1000: 100%|██████████| 281/281 [00:26<00:00, 10.81batch/s, loss=0.0115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012205947531897614 < 0.012210643508892467, epoch: 52 \n",
      "Epoch 52/1000, Avg. Loss: 0.012433177008604749, Val. Loss: 0.012205947531897614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.20batch/s, loss=0.0124]\n",
      "Validation Epoch 53/1000: 100%|██████████| 281/281 [00:25<00:00, 10.86batch/s, loss=0.0113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/1000, Avg. Loss: 0.012438012320242654, Val. Loss: 0.012279720108598152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.19batch/s, loss=0.0124]\n",
      "Validation Epoch 54/1000: 100%|██████████| 281/281 [00:26<00:00, 10.69batch/s, loss=0.0121]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/1000, Avg. Loss: 0.01243888231672072, Val. Loss: 0.012218968674301889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.21batch/s, loss=0.0124]\n",
      "Validation Epoch 55/1000: 100%|██████████| 281/281 [00:26<00:00, 10.68batch/s, loss=0.0118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/1000, Avg. Loss: 0.012432998527443827, Val. Loss: 0.012208329533554906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/1000: 100%|██████████| 1682/1682 [04:30<00:00,  6.21batch/s, loss=0.0124]\n",
      "Validation Epoch 56/1000: 100%|██████████| 281/281 [00:25<00:00, 10.86batch/s, loss=0.0114]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01220454051891786 < 0.012205947531897614, epoch: 56 \n",
      "Epoch 56/1000, Avg. Loss: 0.012435250590011637, Val. Loss: 0.01220454051891786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/1000: 100%|██████████| 1682/1682 [04:30<00:00,  6.21batch/s, loss=0.0124]\n",
      "Validation Epoch 57/1000: 100%|██████████| 281/281 [00:25<00:00, 10.82batch/s, loss=0.0119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/1000, Avg. Loss: 0.012430001386883143, Val. Loss: 0.012223588451875699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/1000: 100%|██████████| 1682/1682 [04:30<00:00,  6.21batch/s, loss=0.0124]\n",
      "Validation Epoch 58/1000: 100%|██████████| 281/281 [00:26<00:00, 10.62batch/s, loss=0.0115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/1000, Avg. Loss: 0.012427704257051173, Val. Loss: 0.01222451924415988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.21batch/s, loss=0.0124]\n",
      "Validation Epoch 59/1000: 100%|██████████| 281/281 [00:26<00:00, 10.73batch/s, loss=0.0125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/1000, Avg. Loss: 0.01242100279811464, Val. Loss: 0.01221617956204983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.20batch/s, loss=0.0124]\n",
      "Validation Epoch 60/1000: 100%|██████████| 281/281 [00:26<00:00, 10.56batch/s, loss=0.0119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012194960520625964 < 0.01220454051891786, epoch: 60 \n",
      "Epoch 60/1000, Avg. Loss: 0.012420265131389698, Val. Loss: 0.012194960520625964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/1000: 100%|██████████| 1682/1682 [04:30<00:00,  6.21batch/s, loss=0.0124]\n",
      "Validation Epoch 61/1000: 100%|██████████| 281/281 [00:26<00:00, 10.61batch/s, loss=0.0118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/1000, Avg. Loss: 0.01243225454577598, Val. Loss: 0.012204971144579059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/1000: 100%|██████████| 1682/1682 [04:30<00:00,  6.21batch/s, loss=0.0124]\n",
      "Validation Epoch 62/1000: 100%|██████████| 281/281 [00:26<00:00, 10.69batch/s, loss=0.0119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/1000, Avg. Loss: 0.012419795951895709, Val. Loss: 0.012205513936609983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.21batch/s, loss=0.0124]\n",
      "Validation Epoch 63/1000: 100%|██████████| 281/281 [00:25<00:00, 10.84batch/s, loss=0.0105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/1000, Avg. Loss: 0.012416784068531889, Val. Loss: 0.01221347789137601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.21batch/s, loss=0.0124]\n",
      "Validation Epoch 64/1000: 100%|██████████| 281/281 [00:26<00:00, 10.46batch/s, loss=0.0131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012190266542163184 < 0.012194960520625964, epoch: 64 \n",
      "Epoch 64/1000, Avg. Loss: 0.012416197854126322, Val. Loss: 0.012190266542163184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.20batch/s, loss=0.0124]\n",
      "Validation Epoch 65/1000: 100%|██████████| 281/281 [00:25<00:00, 10.81batch/s, loss=0.0127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/1000, Avg. Loss: 0.01241142260172049, Val. Loss: 0.01219364742871069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.19batch/s, loss=0.0124]\n",
      "Validation Epoch 66/1000: 100%|██████████| 281/281 [00:25<00:00, 10.89batch/s, loss=0.0118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012188254676600155 < 0.012190266542163184, epoch: 66 \n",
      "Epoch 66/1000, Avg. Loss: 0.012412048003101427, Val. Loss: 0.012188254676600155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.18batch/s, loss=0.0124]\n",
      "Validation Epoch 67/1000: 100%|██████████| 281/281 [00:25<00:00, 10.94batch/s, loss=0.0122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/1000, Avg. Loss: 0.012410368500709286, Val. Loss: 0.012206497257475963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.19batch/s, loss=0.0124]\n",
      "Validation Epoch 68/1000: 100%|██████████| 281/281 [00:26<00:00, 10.72batch/s, loss=0.0117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/1000, Avg. Loss: 0.012410075943053265, Val. Loss: 0.01220995009449241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.19batch/s, loss=0.0124]\n",
      "Validation Epoch 69/1000: 100%|██████████| 281/281 [00:26<00:00, 10.76batch/s, loss=0.0128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/1000, Avg. Loss: 0.012409784138982275, Val. Loss: 0.012222449742655312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.20batch/s, loss=0.0124]\n",
      "Validation Epoch 70/1000: 100%|██████████| 281/281 [00:25<00:00, 10.87batch/s, loss=0.0125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/1000, Avg. Loss: 0.012405869023881046, Val. Loss: 0.012202036089770947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.20batch/s, loss=0.0124]\n",
      "Validation Epoch 71/1000: 100%|██████████| 281/281 [00:25<00:00, 10.87batch/s, loss=0.0124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/1000, Avg. Loss: 0.01240978153991678, Val. Loss: 0.01221894753891378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.20batch/s, loss=0.0124]\n",
      "Validation Epoch 72/1000: 100%|██████████| 281/281 [00:27<00:00, 10.38batch/s, loss=0.0113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/1000, Avg. Loss: 0.012413969626107588, Val. Loss: 0.012212173900570309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73/1000: 100%|██████████| 1682/1682 [04:31<00:00,  6.20batch/s, loss=0.0124]\n",
      "Validation Epoch 73/1000: 100%|██████████| 281/281 [00:26<00:00, 10.76batch/s, loss=0.0126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/1000, Avg. Loss: 0.012400113463105012, Val. Loss: 0.012206023144684865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74/1000: 100%|██████████| 1682/1682 [04:30<00:00,  6.21batch/s, loss=0.0124]\n",
      "Validation Epoch 74/1000: 100%|██████████| 281/281 [00:25<00:00, 10.86batch/s, loss=0.0114]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/1000, Avg. Loss: 0.012396652824007303, Val. Loss: 0.012189609505686896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/1000: 100%|██████████| 1682/1682 [04:30<00:00,  6.21batch/s, loss=0.0124]\n",
      "Validation Epoch 75/1000: 100%|██████████| 281/281 [00:27<00:00, 10.27batch/s, loss=0.012] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/1000, Avg. Loss: 0.012394623320957249, Val. Loss: 0.012216815270907726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76/1000: 100%|██████████| 1682/1682 [04:30<00:00,  6.21batch/s, loss=0.0124]\n",
      "Validation Epoch 76/1000: 100%|██████████| 281/281 [00:25<00:00, 10.83batch/s, loss=0.0128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss did not improve for 10 epochs. Early stopping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_root = '/home/yousef.metwally/projects/data/tomotwin_training_data/training'\n",
    "val_root = '/home/yousef.metwally/projects/data/tomotwin_training_data/validation'\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "logging = '/home/yousef.metwally/projects/mobilenet'\n",
    "dataset = MRCVolumeDataset(dataset_root)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_set = MRCVolumeDataset(val_root)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "model = MobileNetV2Autoencoder()\n",
    "model = nn.DataParallel(model)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "    \n",
    "train_autoencoder(model, data_loader, val_loader, optimizer,  scheduler,logging, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([12, 1, 32, 32, 32])\n",
      "Encoder output after layer 0: torch.Size([12, 32, 32, 16, 16])\n",
      "Encoder output after layer 1: torch.Size([12, 16, 32, 16, 16])\n",
      "Encoder output after layer 2: torch.Size([12, 24, 16, 8, 8])\n",
      "Encoder output after layer 3: torch.Size([12, 24, 16, 8, 8])\n",
      "Encoder output after layer 4: torch.Size([12, 32, 8, 4, 4])\n",
      "Encoder output after layer 5: torch.Size([12, 32, 8, 4, 4])\n",
      "Encoder output after layer 6: torch.Size([12, 32, 8, 4, 4])\n",
      "Encoder output after layer 7: torch.Size([12, 64, 4, 2, 2])\n",
      "Encoder output after layer 8: torch.Size([12, 64, 4, 2, 2])\n",
      "Encoder output after layer 9: torch.Size([12, 64, 4, 2, 2])\n",
      "Encoder output after layer 10: torch.Size([12, 64, 4, 2, 2])\n",
      "Encoder output after layer 11: torch.Size([12, 96, 4, 2, 2])\n",
      "Encoder output after layer 12: torch.Size([12, 96, 4, 2, 2])\n",
      "Encoder output after layer 13: torch.Size([12, 96, 4, 2, 2])\n",
      "Encoder output after layer 14: torch.Size([12, 160, 2, 1, 1])\n",
      "Encoder output after layer 15: torch.Size([12, 160, 2, 1, 1])\n",
      "Encoder output after layer 16: torch.Size([12, 160, 2, 1, 1])\n",
      "Encoder output after layer 17: torch.Size([12, 320, 2, 1, 1])\n",
      "Encoder output after layer 18: torch.Size([12, 1280, 2, 1, 1])\n",
      "Output after avg_pool3d: torch.Size([12, 1280, 1, 1, 1])\n",
      "Output after view: torch.Size([12, 1280, 1, 1, 1])\n",
      "Decoder output after layer 0: torch.Size([12, 320, 2, 2, 2])\n",
      "Decoder output after layer 1: torch.Size([12, 320, 2, 2, 2])\n",
      "Decoder output after layer 2: torch.Size([12, 320, 2, 2, 2])\n",
      "Decoder output after layer 3: torch.Size([12, 160, 4, 4, 4])\n",
      "Decoder output after layer 4: torch.Size([12, 160, 4, 4, 4])\n",
      "Decoder output after layer 5: torch.Size([12, 160, 4, 4, 4])\n",
      "Decoder output after layer 6: torch.Size([12, 96, 8, 8, 8])\n",
      "Decoder output after layer 7: torch.Size([12, 96, 8, 8, 8])\n",
      "Decoder output after layer 8: torch.Size([12, 96, 8, 8, 8])\n",
      "Decoder output after layer 9: torch.Size([12, 64, 16, 16, 16])\n",
      "Decoder output after layer 10: torch.Size([12, 64, 16, 16, 16])\n",
      "Decoder output after layer 11: torch.Size([12, 64, 16, 16, 16])\n",
      "Decoder output after layer 12: torch.Size([12, 32, 32, 32, 32])\n",
      "Decoder output after layer 13: torch.Size([12, 32, 32, 32, 32])\n",
      "Decoder output after layer 14: torch.Size([12, 32, 32, 32, 32])\n",
      "Decoder output after layer 15: torch.Size([12, 1, 32, 32, 32])\n",
      "Decoder output after layer 16: torch.Size([12, 1, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "model = MobileNetV2Autoencoder()\n",
    "out = model(torch.rand(12,1,32,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tomotwin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
